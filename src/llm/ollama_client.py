"""
Ollama client for local LLM processing.

This module provides a client interface for interacting with a local Ollama instance,
which allows running various open-source large language models locally. It abstracts
away the API communication details and provides a simple interface for text generation.
"""

import requests  # For making HTTP requests to the Ollama API
from typing import Dict, List, Any, Optional  # For type hints
import json  # For parsing JSON responses

class OllamaClient:
    """
    Client for interacting with local Ollama instance.
    
    This class provides a simplified interface to the Ollama API,
    handling communication details and providing a clean way to
    generate text, check model availability, and load models.
    """
    
    def __init__(
        self,
        base_url: str = "http://localhost:11434",  # Default Ollama server address
        model: str = "mistral",  # Default to Mistral as it's a good balance of performance and speed
        temperature: float = 0.7  # Default temperature for generation
    ):
        """
        Initialize Ollama client with configuration parameters.
        
        Parameters:
        -----------
        base_url: URL of the Ollama instance
            The address where Ollama server is running
        model: Model to use for text generation
            Examples include 'llama2', 'mistral', 'codellama', 'deepseek-r1'
        temperature: Controls randomness in text generation
            Lower values (e.g., 0.1) are more deterministic
            Higher values (e.g., 1.0) produce more diverse outputs
        """
        # Store base URL with trailing slash removed for consistency
        self.base_url = base_url.rstrip('/')
        self.model = model
        self.temperature = temperature
    
    def generate(
        self,
        system_prompt: str,  # Context/instructions for the LLM
        user_prompt: str,    # The specific query or input
        temperature: Optional[float] = None  # Optional override for default temperature
    ) -> str:
        """
        Generate text using the configured Ollama model.
        
        This method:
        1. Combines system and user prompts in the format expected by Ollama
        2. Sends the request to the Ollama API
        3. Extracts and returns the generated text
        4. Handles errors appropriately
        
        Parameters:
        -----------
        system_prompt: System-level instructions that define the LLM's role and behavior
            Sets context and constraints for the generation
        user_prompt: The specific input or query from the user
            The actual content the LLM should respond to
        temperature: Optional override for the default temperature setting
            Allows adjusting randomness per request
            
        Returns:
        --------
        str: The text generated by the LLM
        
        Raises:
        -------
        ConnectionError: If cannot connect to Ollama server
        ValueError: If Ollama returns an invalid response
        """
        # Construct the API endpoint URL
        url = f"{self.base_url}/api/generate"
        
        # Combine system and user prompts in the format expected by Ollama
        # The system prompt provides context, and the user prompt is the specific query
        prompt = f"{system_prompt}\n\nUser: {user_prompt}"
        
        # Prepare the request data
        data = {
            "model": self.model,  # Which model to use
            "prompt": prompt,  # The combined prompt
            "temperature": temperature or self.temperature,  # Use provided temperature or default
            "stream": False  # Get complete response at once rather than streaming
        }
        
        try:
            # Send request to Ollama API
            response = requests.post(url, json=data)
            # Raise an exception for HTTP errors
            response.raise_for_status()
            # Extract and return just the generated text portion
            return response.json()["response"]
        except requests.exceptions.RequestException as e:
            # Handle network/connection errors
            raise ConnectionError(f"Failed to connect to Ollama: {e}")
        except (KeyError, json.JSONDecodeError) as e:
            # Handle response parsing errors
            raise ValueError(f"Invalid response from Ollama: {e}")
    
    def is_available(self) -> bool:
        """
        Check if Ollama is available and the configured model is loaded.
        
        This method verifies:
        1. The Ollama server is running and accessible
        2. The specific model requested is available for use
        
        Returns:
        --------
        bool: True if Ollama is available and model is loaded, False otherwise
        """
        try:
            # Check available models endpoint
            url = f"{self.base_url}/api/tags"
            response = requests.get(url)
            response.raise_for_status()
            # Extract list of available models
            models = response.json().get("models", [])
            # Check if our model is in the list
            return self.model in [m["name"] for m in models]
        except:
            # Any exception means Ollama is not properly available
            return False
    
    def load_model(self) -> bool:
        """
        Ensure the configured model is loaded and ready for use.
        
        This method:
        1. Checks if the model is already available
        2. If not, attempts to download/load it
        
        Returns:
        --------
        bool: True if model is successfully loaded or already available,
              False if loading fails
        """
        # First check if model is already available
        if self.is_available():
            return True
            
        try:
            # If not available, try to download/pull the model
            url = f"{self.base_url}/api/pull"
            data = {"name": self.model}
            response = requests.post(url, json=data)
            response.raise_for_status()
            return True
        except:
            # Return False if model loading fails
            return False 